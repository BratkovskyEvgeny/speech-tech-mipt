{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled9.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNcjxo9qwNyYiRhrTtAwTdz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karpnv/speech-tech-mipt/blob/main/week06/spotter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DLqUa76aAHIg",
        "outputId": "82a359d9-8176-4668-d502-fe0861beab9a"
      },
      "source": [
        "!apt install sox"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "sox is already the newest version (14.4.2-3ubuntu0.18.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eX_UvnL9FOB2"
      },
      "source": [
        "### Baseline commands recognition (2-5 points)\n",
        "\n",
        "We're now going to train a classifier to recognize voice. More specifically, we'll use the [Speech Commands Dataset] that contains around 30 different words with a few thousand voice records each."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvHkw2rfY9k7"
      },
      "source": [
        "import os\n",
        "from IPython.display import display, Audio\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import librosa\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "datadir = \"speech_commands\"\n",
        "\n",
        "!wget http://download.tensorflow.org/data/speech_commands_v0.01.tar.gz -O speech_commands_v0.01.tar.gz\n",
        "# alternative url: https://www.dropbox.com/s/j95n278g48bcbta/speech_commands_v0.01.tar.gz?dl=1\n",
        "!mkdir {datadir} && tar -C {datadir} -xvzf speech_commands_v0.01.tar.gz 1> log\n",
        "\n",
        "samples_by_target = {\n",
        "    cls: [os.path.join(datadir, cls, name) for name in os.listdir(\"./speech_commands/{}\".format(cls))]\n",
        "    for cls in os.listdir(datadir)\n",
        "    if os.path.isdir(os.path.join(datadir, cls))\n",
        "}\n",
        "print('Classes:', ', '.join(sorted(samples_by_target.keys())[1:]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ME4cVShQ916w"
      },
      "source": [
        "!sox --info speech_commands/bed/00176480_nohash_0.wav"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cvF5l-PCyd8z",
        "outputId": "9fb07012-a196-4400-dad4-8fbe88a83daf"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from itertools import chain\n",
        "from tqdm import tqdm\n",
        "import joblib as jl\n",
        "\n",
        "classes = (\"left\", \"right\", \"up\", \"down\", \"stop\")\n",
        "\n",
        "def preprocess_sample(filepath, max_length=150):\n",
        "    amplitudes, sr = librosa.core.load(filepath)\n",
        "    spectrogram = librosa.feature.melspectrogram(amplitudes, sr=sr)[:, :max_length]\n",
        "    spectrogram = np.pad(spectrogram, [[0, 0], [0, max(0, max_length - spectrogram.shape[1])]], mode='constant')\n",
        "    target = classes.index(filepath.split(os.sep)[-2])\n",
        "    return np.float32(spectrogram), np.int64(target)\n",
        "\n",
        "all_files = chain(*(samples_by_target[cls] for cls in classes))\n",
        "spectrograms_and_targets = jl.Parallel(n_jobs=-1)(tqdm(list(map(jl.delayed(preprocess_sample), all_files))))\n",
        "X, y = map(np.stack, zip(*spectrograms_and_targets))\n",
        "X = X.transpose([0, 2, 1])  # to [batch, time, channels]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 11834/11834 [05:14<00:00, 37.66it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Ol6sywTG_Y9"
      },
      "source": [
        "batch_size = 16\n",
        "\n",
        "tensor_x = torch.Tensor(X_train)\n",
        "tensor_y = torch.Tensor(y_train)\n",
        "\n",
        "train_dataset = TensorDataset(tensor_x, tensor_y)\n",
        "\n",
        "tensor_x = torch.Tensor(X_test) # transform to torch tensor\n",
        "tensor_y = torch.Tensor(y_test)\n",
        "\n",
        "test_dataset = TensorDataset(tensor_x, tensor_y)\n",
        "\n",
        "\n",
        "trainloader = DataLoader(train_dataset, batch_size=batch_size,\n",
        "                         shuffle=True, num_workers=2)\n",
        "testloader = DataLoader(test_dataset, batch_size=batch_size,\n",
        "                        shuffle=False, num_workers=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-qr8t6wCF8vT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # TODO: define your layers here\n",
        "\n",
        "    def forward(self, x):\n",
        "        # TODO: apply your layers here\n",
        "        return x\n",
        "\n",
        "\n",
        "net = Net()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCZ7MkvsF9gs"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xR1uxQ-GGGLr"
      },
      "source": [
        "for epoch in range(2):  # loop over the dataset multiple times\n",
        "\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        outputs = net(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        running_loss += loss.item()\n",
        "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
        "            print('[%d, %5d] loss: %.3f' %\n",
        "                  (epoch + 1, i + 1, running_loss / 2000))\n",
        "            running_loss = 0.0\n",
        "\n",
        "print('Finished Training')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojEuXjx5DlDW"
      },
      "source": [
        "Train a model: finally, lets' build and train a classifier neural network. You can use any library you like. If in doubt, consult the model & training tips below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwgnOrZy1E8p"
      },
      "source": [
        "__Training tips:__ here's what you can try:\n",
        "* __Layers:__ 1d or 2d convolutions, perhaps with some batch normalization in between;\n",
        "* __Architecture:__ VGG-like, residual, highway, densely-connected, MatchboxNet, Dilated convs - you name it :)\n",
        "* __Batch size matters:__ smaller batches usually train slower but better. Try to find the one that suits you best.\n",
        "* __Data augmentation:__ add background noise, faster/slower, change pitch;\n",
        "* __Average checkpoints:__ you can make model more stable with [this simple technique (arxiv)](https://arxiv.org/abs/1803.05407)\n",
        "* __For full scale stage:__ make sure you're not losing too much data due to max_length in the pre-processing stage!\n",
        "\n",
        "These are just recommendations. As long as your model works, you're not required to follow them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvf8UCsPDvj2"
      },
      "source": [
        "### Full scale commands recognition (3+ points)\n",
        "\n",
        "Your final task is to train a full-scale voice command spotter and apply it to a video:\n",
        "1. Build the dataset with all 30+ classes (directions, digits, names, etc.)\n",
        "  * __Optional:__ include a special \"noise\" class that contains random unrelated sounds\n",
        "  * You can download youtube videos with [`youtube-dl`](https://ytdl-org.github.io/youtube-dl/index.html) library.\n",
        "2. Train a model on this full dataset. Kudos for tuning its accuracy :)\n",
        "3. Apply it to a audio/video of your choice to spot the occurences of each keyword\n",
        " * Here's one [video about primes](https://www.youtube.com/watch?v=EK32jo7i5LQ) that you can try. It should be full of numbers :)\n",
        " * There are multiple ways you can analyze the performance of your network, e.g. plot probabilities predicted for every time-step. Chances are you'll discover something useful about how to improve your model :)\n",
        "\n",
        "\n",
        "Please briefly describe what you did in a short informal report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "16Ux38uFD2g-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}